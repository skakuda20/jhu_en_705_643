"""
Module: run.py

This module is the main entry point for training or evaluating a video classification model.
It uses command-line arguments to configure the experiment, including dataset paths,
model parameters, and training hyperparameters. Depending on the selected mode
('train' or 'eval'), it performs the following:

- Train mode:
    - Loads the dataset from a directory structure.
    - Splits the dataset into training, validation, and test sets.
    - Creates PyTorch Datasets and DataLoaders for training and validation.
    - Initializes the model (e.g., LRCN) and defines the loss function, optimizer,
        and learning rate scheduler.
    - Runs the training loop and saves the best model weights.

- Eval mode:
    - Loads the pre-generated dataset splits.
    - Creates a DataLoader for the test set.
    - Loads the trained model checkpoint.
    - Evaluates the model on the test set and prints overall test accuracy.

The module also includes a helper function for parsing command-line arguments.
"""

import os
import argparse
from pathlib import Path


import torch
from torch import nn
from torch import optim
from torch.optim.lr_scheduler import ReduceLROnPlateau

from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib
from matplotlib import pyplot as plt


from video_datasets import VideoDataset, load_dataset, dataset_split
from utils import (
    transform_stats,
    compose_data_transforms,
    train_val_dloaders,
    test_dloaders,
    get_project_root_path,
)
from models import LRCN
from train import train
from test import test, get_test_report, get_confusion_matrix


matplotlib.use("TkAgg")


def args_parser():
    """
    Parse command-line arguments for configuring the video classification training or evaluation.

    Returns:
        argparse.Namespace: Parsed command-line arguments.

    Arguments include:
        -fd/--frame_dir: Directory for storing video frames.
        -trs/--train_size: Proportion of data to use for training (default 0.7).
        -tss/--test_size: Proportion of data to use for testing (default 0.1).
        -fpv/--fr_per_vid: Number of frames per video to consider (default 16).
        -nc/--n_classes: Number of classes for the classification task (required).
        -c/--ckpt: Path for loading a trained model checkpoint.
        -mt/--model_type: Model type, either '3dcnn' or 'lrcn' (default 'lrcn').
        -cnn/--cnn_backbone: Backbone CNN for 2D feature extraction (default 'resnet34').
        -p/--pretrained: Whether to use a pretrained CNN backbone (default True).
        -rhs/--rnn_hidden_size: Number of neurons in the RNN/LSTM hidden layer (default 100).
        -rnl/--rnn_n_layers: Number of RNN/LSTM layers (default 1).
        -m/--mode: Mode of operation: 'train' or 'eval' (required).
        -bs/--batch_size: Mini-batch size (required).
        -d/--dropout: Dropout rate for regularization (default 0.1).
        -lr/--learning_rate: Learning rate for training (default 3e-5).
        -ne/--n_epochs: Number of training epochs (default 30).
    """
    parser = argparse.ArgumentParser(description="Video Classification Training")

    parser.add_argument("-fd", "--frame_dir", help="Directory for storing video frames")
    parser.add_argument(
        "-trs", "--train_size", type=float, default=0.7, help="Train set size"
    )
    parser.add_argument(
        "-tss", "--test_size", type=float, default=0.1, help="Test set size"
    )
    parser.add_argument(
        "-fpv", "--fr_per_vid", type=int, default=16, help="Number of frames per video"
    )
    parser.add_argument(
        "-nc",
        "--n_classes",
        type=int,
        help="Number of classes for the classification task",
        required=True,
    )

    parser.add_argument(
        "-c", "--ckpt", help="Path for loading trained model checkpoints"
    )
    parser.add_argument("-mt", "--model_type", help="3D CNN or LRCN", default="lrcn")
    parser.add_argument(
        "-cnn",
        "--cnn_backbone",
        default="resnet34",
        help="2D CNN backbone - options: resnet18, resnet34, resnet50, resnet101, resnet152",
    )
    parser.add_argument(
        "-p", "--pretrained", help="Use pretrained 2D CNN backbone", default=True
    )
    parser.add_argument(
        "-rhs",
        "--rnn_hidden_size",
        type=int,
        default=100,
        help="Number of neurons in the RNN/LSTM hidden layer",
    )
    parser.add_argument(
        "-rnl", "--rnn_n_layers", type=int, default=1, help="Number of RNN/LSTM layers"
    )

    parser.add_argument(
        "-m",
        "--mode",
        type=str,
        default="train",
        help="Either 'train' or 'eval'",
        required=True,
    )
    parser.add_argument(
        "-bs", "--batch_size", type=int, help="Mini-batch size", required=True
    )
    parser.add_argument(
        "-d",
        "--dropout",
        type=float,
        default=0.1,
        help="Dropout rate for regularization",
    )
    parser.add_argument(
        "-lr",
        "--learning_rate",
        type=float,
        default=3e-5,
        help="Learning rate for model training",
    )
    parser.add_argument(
        "-ne", "--n_epochs", type=int, default=30, help="Number of training epochs"
    )

    args = parser.parse_args()
    return args


def main(args):
    """
    Main function to execute training or evaluation based on the parsed command-line arguments.

    For training:
        - Loads the dataset from the specified frame directory.
        - Splits the dataset into training, validation, and test sets.
        - Saves the splits for later use.
        - Creates the training and validation DataLoaders.
        - Initializes the model (LRCN) with specified hyperparameters.
        - Defines the loss function, optimizer, and learning rate scheduler.
        - Runs the training loop and saves the best model weights.

    For evaluation:
        - Loads the saved dataset splits.
        - Creates the test DataLoader.
        - Loads the trained model checkpoint.
        - Evaluates the model on the test set and prints the overall test accuracy.

    Args:
        args (argparse.Namespace): Parsed command-line arguments.
    """
    # Dataset parameters
    frame_dir = args.frame_dir
    tr_size = args.train_size
    ts_size = args.test_size
    fr_per_vid = args.fr_per_vid
    n_classes = args.n_classes

    # Model parameters
    model_type = args.model_type
    rnn_hidden_size = args.rnn_hidden_size
    rnn_n_layers = args.rnn_n_layers
    dropout = args.dropout
    pretrained = args.pretrained
    cnn_backbone = args.cnn_backbone
    # ckpt = args.ckpt

    # Training parameters
    mode = args.mode
    batch_size = args.batch_size
    n_epochs = args.n_epochs
    learning_rate = args.learning_rate
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    # Load transformation statistics and create data augmentation transforms
    h, w, mean, std = transform_stats(model_type)
    tr_transforms, val_ts_transforms = compose_data_transforms(h, w, mean, std)

    # Initialize the model (LRCN)
    model = LRCN(
        hidden_size=rnn_hidden_size,
        n_layers=rnn_n_layers,
        dropout_rate=dropout,
        n_classes=n_classes,
        pretrained=pretrained,
        cnn_model=cnn_backbone,
    )

    if mode == "train":
        # Load dataset and split into train/validation/test
        vid_dataset, label_dict = load_dataset(frame_dir)
        tr_split, val_split, ts_split = dataset_split(vid_dataset, tr_size, ts_size)

        # Save the splits for reproducibility and later use in evaluation
        splits = {
            "train": np.array(tr_split),
            "val": np.array(val_split),
            "test": np.array(ts_split),
        }
        np.save("./splits.npy", splits)

        # Create PyTorch Datasets and DataLoaders for train and validation
        tr_dataset = VideoDataset(tr_split, fr_per_vid, tr_transforms)
        val_dataset = VideoDataset(val_split, fr_per_vid, val_ts_transforms)
        dataloaders = train_val_dloaders(
            tr_dataset, val_dataset, batch_size, model_type
        )

        # Define the loss function, optimizer, and learning rate scheduler
        loss_func = nn.CrossEntropyLoss(reduction="sum")
        opt = optim.Adam(model.parameters(), lr=learning_rate)
        lr_scheduler = ReduceLROnPlateau(
            opt, mode="min", factor=0.5, patience=5, verbose=1
        )
        os.makedirs("./models", exist_ok=True)
        optim_model_dir = "./models"

        # Main training procedure
        model.to(device)
        model, _, _ = train(
            dataloaders,
            model,
            loss_func,
            opt,
            lr_scheduler,
            device,
            optim_model_dir,
            n_epochs,
        )

    elif mode == "eval":
        # Load saved dataset splits
        splits = np.load("./splits.npy", allow_pickle=True)
        ts_split = splits.item()["test"]
        ts_split = [(sample[0], int(sample[1])) for sample in ts_split]

        # Create PyTorch Dataset and DataLoader for the test set
        ts_dataset = VideoDataset(ts_split, fr_per_vid, val_ts_transforms)
        dataloaders = test_dloaders(ts_dataset, batch_size, model_type)

        # Load the trained model checkpoint
        model.load_state_dict(torch.load(args.ckpt))
        model.to(device)
        targets, outputs, accuracy, y_scores = test(model, dataloaders["test"], device)

        # Get target names:
        _, label_dict = load_dataset(frame_dir)

        print(f"The overall test accuracy is {100 * accuracy:.4f}%.")

        # Optionally, generate a detailed test report or confusion matrix:
        report_dict = get_test_report(targets, outputs, list(label_dict.keys()))
        print("Weighted Average:", report_dict["weighted avg"])

        report_df = (
            pd.DataFrame(report_dict).transpose().drop(["accuracy"], errors="ignore")
        )
        root_path = get_project_root_path()
        heatmap_path = Path(root_path, "output", "heatmap.png")
        plt.figure(figsize=(9, 15))
        sns.heatmap(report_df.iloc[:-2, :-1], annot=True, cmap="Blues", fmt=".2f")
        plt.title("Classification Report Heatmap")
        plt.tight_layout()
        plt.savefig(heatmap_path)

        roc_auc = roc_auc_score(outputs, y_scores, multi_class="ovr")
        print("ROC AUC Score:", roc_auc)

        pr_auc = average_precision_score(outputs, y_scores, average="macro")
        print("PR AUC Score:", pr_auc)

        print(
            get_confusion_matrix(targets, outputs, label_dict, list(label_dict.keys()))
        )

    else:
        raise ValueError('The mode argument must be either "train" or "eval".')


if __name__ == "__main__":
    args = args_parser()
    main(args)
